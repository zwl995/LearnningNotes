# Weekly Papers
----------------------------------------------------

- [x] [**ICCV 2021** | **[Github](https://github.com/ChrisWu1997/DeepCAD)**] [DeepCAD: A Deep Generative Network for Computer-Aided Design Models](https://arxiv.org/pdf/2105.09492.pdf)


> This paper presents a deep generative method that outputs the sequence of operations used in CAD tools. DeepCAD considers the most commonly used CAD operations, and unifies them into a common structure that encodes the operation types, parameters and sequential orders. It uses a Transformer-based autoencoder to embed the CAD model into a latent space, and decodes the latent vector into a CAD command sequence. This work also generates a large amount of CAD models by Onshape to train their autoencoder. (The CAD model has two levels of representation: one is the user-interaction level, one the the boundary representation(Brep).)
In DeepCAD, a CAD model is described by a sequence of curve commands interleave with extrusion commands. To overcome the issue that CAD commands differ from the natural language, DeepCAD represents each command into a 16-dimensional vectors, and also fixed the total number of commands in every CAD model. It also unified continuous and discrete parameters by quantizing the continuous parameters, which can improve the generation quality. Leveraging the representation of CAD commands, the autoencoder takes as input a CAD command sequence, and projects commands into the embedding space, then feds the embeddings into an encoder to output a latent vector. At last, DeepCAD used the latent-GAN to automatically generate editable CAD models. The proposed method is benchmarked based on the Command Accuracy (ACCcmd) and Parameter Accuracy (ACCparam). 
The experiments also show the effectiveness the data representation. However, this method performs not good on CAD models that require long commands to describe.

Q: Why failed when command sequences becomes quite long?
A: Not enough training data for long sequences.

- [x] [**CVPR 2021** | **[Github]()**] [uncertainty guided collaborative training for weakly supervised temporal action detection](https://openaccess.thecvf.com/content/CVPR2021/papers/Yang_Uncertainty_Guided_Collaborative_Training_for_Weakly_Supervised_Temporal_Action_Detection_CVPR_2021_paper.pdf)

> This paper deals with the temporal action detection problem. The work proposed an ***Uncertainty Guided Collaborative Training(UGCT)*** strategy that enables the online generation of pseudo ground truth labels and an uncertainty aware learning module that can mitigate the noise in the generated pseudo labels. In the pseudo ground truth generation module, the pseudo labels are generated by a teacher model. The RGB stream and FLOW stream work collaboratively that can learn from each other, as the teacher model in the RGB stream provides pseudo labels for the FLOW stream and the teacher model in the FLOW stream provides pseudo labels for the RGB stream. In the uncertainty prediction module, the uncertainty about the pseudo label is predicted. And this module is removed after training.


- [x] [**ICCV 2021 oral** | **[Github](https://github.com/autonomousvision/unisurf)**] [UNISURF: Unifying Neural Implicit Surfaces and Radiance Fields for Multi-View Reconstruction](http://www.cvlibs.net/publications/Oechsle2021ICCV.pdf)

> This work deals with the neural implicit rendering problem.
The rendering technique of ***neural implicit multi-view reconstruction*** has two mainstream catagories: *surface rendering technique* and *volume rendering technique*. Surface rendering technique can estimate surface geometry accurately, while requires per-pixel object masks and appropriate network initialization, which is limited to small scenes. Volume rendering techniques like NeRF require no input masks, while can only extract surface as level sets of the underlying volume density, which are usually non-smooth and contain artifacts.
UNISURF proposed an unified formulation which combines both the advantages of surface rendering and volume rendering, enabling the reconstruction of accurate geometry from multi-view images without masks. And it adopted a $l_1$ reconstruction loss with a $l_2$ surface regularization term. During optimization, the sampling interval for drawing samples during volume rendering is monotonically decreased. UNISURF is evaluated on the DTU, blended MVS datasets and SceneNet dataset, and achieves state-of-the-art. However, this work is limited on solid and non-transparent surfaces. Besides, the overexposed and texture-less areas can also limit UNISURF's performance, and the reconstructions are less accurate at rarely visible regions in the images. (My question: can this method adapts to larger areas, i.e, city-scale aerial images?)

- [x] [**ECCV 2020**] [Unsupervised Shape and Pose Disentanglement
for 3D Meshes](https://arxiv.org/pdf/2007.11341.pdf)

> The paper addresses the problem of unsupervised disentanglement of
pose and shape for 3D meshes. It used an auto-encoder neural network to auto-encode a mesh in *pose* code and *shape* code. Built upon the cross-consistency and self-consistency. To achieve the cross-consistency constraints, this work generates pairs of different shapes with the exact same pose on the fly during training with our disentangling network. The self-consistency is achieved by generating a proxy mesh $\tilde{X}^t$ with the pose of mesh $X^s_1$ and the shape of mesh $X_t$ within the training loop from two meshes with different shapes and poses $X^s_1$ and $X^t$. An effective disentanglement should recover the original pose code from the proxy mesh. Accordingly, the cross-consistency loss and self-consistency loss are proposed to train the network. At last, this work used the *as-rigid-as-possible Deformation* algorithm to deform $X^t$ to match the pose of the network prediction $\tilde{X}^t$ while preserving the original shape as much as possible. Their model is straightforward to train and it generalizes well across various datasets.
The limitation is that they have no control over specific parts of the mesh with their pose code. And also the interpolation of large torso rotations squeezes the meshes.

- [x] [**NeurIPS 2021** | **[Github](https://github.com/TorchSSL/TorchSSL)**] [FlexMatch: Boosting Semi-Supervised Learning with Curriculum Pseudo Labeling]()

> This paper deals with the semi-supervised learning(SSL) problem. The main contribution of this work is to propose the Curriculum Pseudo Labeling(CPL) approach, which takes into account the learning status of each class for semi-supervised learning. As modern SSL algorithms such as Pseudo-Labeling and Unsupervised Data Augmentation(UDA) rely on a fixed threshold to compute the unsupervised loss, and can only handle all classes equally without considering the different learning difficulties, this work used a initial threshold which can be dynamically adjusted during training. when apply the CPL module into FixMatch, the called FlexMatch method converges faster than FixMatch and achieves SOTA performances on most SSL image classification benchmarks. Specifically, FixMatch uses consistency regularization and data augmentation to improve the training performance, where the unsupervised loss needs a pre-defined threshold. The intuition of Curriculum Pseudo Labeling is that, when the threshold is high, the learning effect of a class can be reflected by the number of samples whose predictions fall into this class and above the threshold. Based on the intuition, CPL proposed a new loss, where the threshold of a well-learned class is raised higher to selectively pick up higher-quality samples as learning proceeds. Besides, more techniques such as threshold warm-up and nonlinear mapping are proposed to improve the performance.


- [x] [**NeurIPS 2021**] [Adversarial Teacher-Student Representation Learning for Domain Generalization](https://papers.nips.cc/paper/2021/file/a2137a2ae8e39b5002a3f8909ecb88fe-Paper.pdf)

> This work aims at solving the **domain generalization** problem. Different from domain adaptation (DA), which observes both source
and target-domain training data for performing learning tasks across domains, domain generalization trains models using data observed from single or multiple source domains to expect generalizing the model to unseen target domains. Unlike most existing domain generalization approaches, which focus on deriving **domain-invariant features** among multiple source domains or adopting meta-learning techniques, this work proposed a unique Adversarial Teacher-Student Representation Learning framework. The core parts of the method are the **Domain Generalized Representation Learning** module and the **Novel Domain Augmentation** module. In the Domain Generalized Representation Learning module, a teacher-Student learning scheme is proposed to extract the domain generalizable feature representations, where the Student is used for exploring novel-domain augmentation synthesized from the novel-domain
augmenter G, while distilling the associated knowledge to update Teacher, and the teacher can gain more generalizability.  The Novel Domain Augmentation is used to progressively perform novel-domain data augmentation. The aim of this module is to train the novel-domain augmenter G while freezing both teacher and student, so as to maximize the discrepancy between z and $\tilde{z}$. The experiment is conducted on the PACS, Office-Home, Domain-Net, VLCS and Digit-DG datasets, and retrieve remarkably improvement compared to existing methods. The ablation study also shows the effectiveness of the Adversarial Augmenter and the Domain Generalized Teacher.


- [x] [**CVPR 2021**] [Neural Scene Graphs for Dynamic Scenes](http://light.princeton.edu/neural-scene-graphs)

> This work deals with the neural rendering problem under dynamic scenes. Existing representative works like NeRF can render static scenes, however, are not able to render dynamic scenes and decompose scenes into individual objects. To solve the problem, this work represents scenes by a learned scene graph, where the static objects and dynamics objects are nodes, and transformations and scaling factors are edges. Based on NeRF, this work proposes to augment the implicit neural radiance fields when renders background node and dynamic nodes, where the original positional encoding of inputs is replaced by a Fourier encoding. For background node, the static radiance is represented on sparse planes instead of a volume. For dynamic nodes, objects belong to the same class are represented by a function with same weights to reduce the number of models and also the training time.. Meanwhile, the neural radiance fields is augmented with a ***latent class encoding*** to distinguish individual objects. Besides, the pose is also served as input to the neural radiance fields to ensure volumetric consistency.
At last, the learned scene graph is rendered using a ray casting approach.
To increase efficiency, the sampling at the static node is limited to $N_s$ planes that are parallel to the image plane. For each dynamic node, each ray from the camera for intersections with all dynamic nodes is checked by translating the ray to an object local frame and applying an efficient AABB-ray intersection test. Besides, only a small number of equidistant points 
are enough to accurately represent dynamic objects and maintaining rendering efficiency.
The learned scene graph is optimized based on the predicted color with the latent codes served as regularizers. The experiments are conducted on the KITTI dataset and the virtual KITTI 2 dataset. When compared against the SRN method, NeRF and NeRF with time parameter, this work shows superiority over these methods under dynamic scenes.

- [x] [**NeurIPS 2021**] [H-NeRF: Neural Radiance Fields for Rendering and Temporal Reconstruction of Humans in Motion](https://openreview.net/pdf?id=s-NI4H4e3Rf)

> The task of this paper is to learn both the detailed temporal geometry of the human in motion, and to render the sequence from novel camera views and for different human poses, by given a collection of images capturing a dynamic scene of a human in motion.
The basic idea of this paper is to unify the volumetric radiance fields and the implicit signed distance function, which takes a similar idea of the ICCV 2021 paper: UNISURF, which unifies the implicit surface representation and volumetric radiance fields. This paper firstly proposed a static semantic human NeRF, where the coarse neural radiance fields structure is constrained by a 3D bounding box generated from imGHUM, and the SDF if co-learned with NeRF, where the alpha value is replaced by a combination of original alpha value and a pseudo alpha value. The pseudo alpha value is the signed distance after applying Sigmoid activation function. Besides, a residual SDF network is also used to update the signed distance. Then, the static human NeRF is extended to dynamic scenes. To implicitly model scenes where the human moves, they learned a consistent, continuous function for both the geometry and the appearance variations across frames, where the continuous function is conditioned on a semantically meaningful latent code $\mathbf{z}$. The pose latent code of a spatial point is mapped to a 4-dimension canonical point descriptor, and this descriptor is used as the input of human NeRF. To obtain a fine-grained human shape, the volume density and color value are further conditioned on the human pose and the body pose code.

- [x] [**3DV 2021**] [Generative Zero-Shot Learning for Semantic Segmentation of 3D Point Clouds]()

> This work deals with the generalized zero-shot learning problem for 3D points clouds data, where seen classes and unseen classes can appear in a scene simultaneously. Most of the zero shot learning methods focus on the classification problem, while little tackles the semantic segmentation, this work is the first one to handle both problems. This method contains three core parts: a backbone feature extractor, a generator conditioned on class prototypes and a final classifier for both seen and unseen classes.
The backbone is used to extract features from point clouds. For classification, the object $P_i$ is the point cloud; for semantic segmentation, $P_i$ is the point cloud containing the point to label. Firstly, the backbone is trained on seen classes, then the generator G(·) is used to create a training set as unseen object representations and to train a final classifier for unseen classes. The input to this generator is a random vector and a seen class prototype. The final classifier is trained on generated features for zero-shot learning, and for generalized zero-shot learning, it's trained on both seen and unseen classes. As the training is conducted on seen classes, the bias towards seen classes is tackled by a class-dependent weighting, where the weighting factor for unseen classes is greater than one. Besides, a calibrated stacking is used to reduce the prediction probability for seen classes. The method is evaluated on the ModelNet40 dataset for classification, and the S3DIS, the ScanNet and the SemanticKITTI dataset for semantic segmentation. The experiments shows this method outperforms the baseline by a very large margin.

- [x] [**ICCV 2021**] [Hierarchical Conditional Flow: A Unified Framework for Image Super-Resolution and Image Rescaling]()

> This work proposed a unified neural network that can deal with the the image super-resolution task and the image rescaling task. Given low resolution image, image super resolution aims at reconstructing the high resolution image; while image rescaling is aimed to downscale the high resolution image to low resolution image, meanwhile, the information from the high resolution image is maintained for better subsequent reconstruction, Two representative works are respectively the SRFlow and IRN. This work models these two processes with an invertible bijective transformation. In the forward pass, the high resolution image is hierarchically decomposed into a low resolution image and a latent variable at each level. In inverse propagation, yl and al are computed from level L to level 1. More specifically, the network architecture contains a squeeze layer and K flow-steps at each level, each flow-step consists of a an Actnorm layer, an invertible 1 × 1 convolution layer and an affine coupling layer. After that, a split layer is used to decompose the tensor into a low resolution image $y_l$ and a latent variable $a_l$. Then, $y_l$ is fed to the next level, while $a_l$ is transformed to the latent variable $z_l$ by P flow-steps. For image super resolution task, the loss function is composed of a negative log-likelihood loss, a $L_1$ pixel loss, a perceptual loss and a GAN loss. For the image rescaling task, the loss is composed of two $L_1$ pixel losses on the high resolution image and low resolution image respectively and a loss on the latent variable. The experiments are conducted on both image super resolution and image rescaling, the effectiveness of each loss is also evaluated on the experiments.

- [x] [**CVPR 2022**] [GroupViT: Semantic Segmentation Emerges from Text Supervision](https://arxiv.org/pdf/2202.11094.pdf)

> This work proposed a hierarchical Grouping Vision Transformer (GroupViT) method which allows semantic segmentation to be trained with only text supervision. Instead of operating on regular image grids, this work learns to group image regions into progressively larger arbitrary shaped segments. By using transformer, the global self attention mechanism of Transformers naturally provides the flexibility to combine visual tokens into non-grid-like segments. This work performs hierarchical grouping of visual tokens into irregular-shaped segments. Specifically, in GroupViT, the Transformer layers are separated into multiple grouping stages. In each stage, a number of group tokens are learned via self-attention which aggregates information globally from all image segments. Then these learned group tokens are used to merge similar image tokens via a Grouping Block. In this way, the network can produce larger and less segments. After the final grouping stage, Transformer layers are applied on all segment tokens and outputs are input to the average pooling layer to obtain the final global image representation. To perform learning, this image-level embedding is compared to those derived from textual sentences via contrastive learning, where the image-text contrastive loss and the multi-label image-text contrastive loss are used for training. During inference, the image is extracted to segments, and the label names are extracted to textual embedding. Then the segmentation labels are assigned to image segments according to their mutual similarity in the embedding space. For experiments, GroupViT is trained on the Conceptual 12 Million (CC12M) and Yahoo Flickr Creative Commons] datasets with text supervision alone, can transfer to semantic segmentation tasks on the PASCAL VOC and PASCAL Context datasets in a zero-shot manner. 

- [x] [**arXiv 2022**] [MINI: Mining Implicit Novel Instances for Few-Shot Object Detection]()

> This work deals with the Few-Shot Object Detection (FSOD) problem. Different from previous Few-Shot Object Detection methods, which pre-train the object detector on the base classes with abundant data and then is transferred with few shot training samples to detect novel classes, this work (named MINI) proposed to mine implicit novel instances from base datasets. MINI firstly adopts TFA as an initial miner, where the box predictors are jointly trained on the base dataset with abundant annotations of base classes, then TFA is fine-tuned on a small balanced training set that includes both base and novel classes. Secondly, MINI uses the initialized FSOD model to mine implicit novel instances from base dataset, where a set of proposals are predicted, the feature embedding of these proposals are used to compute the cosine similarity with the features learned from a self-supervised discriminator. The scores predicted by the FSOD model are weighted with the cosine similarities, and are used to adaptively filter the initial proposals. At last, an online mining module is proposed to improve the model's performance. The online mining module adopts a teacher-student framework, where the teach shares the same network architecture with the student and its parameters are updated by exponential moving average of the student's parameters. During training, the teacher first online mines novel instances with a similar procedure with the FSOD model, then the online mined novel instances are mingled with the offline mined novel instances. Besides, a IoU branching correction is further used to improve the quality of the online mined novel instances. The experiments conducted on PASCAL VOC and MS COCO show the method is superior to other Few-Shot Object Detection and semi-supervised object detection methods by a large margin.

- [x] [**2022**] [Learning to Estimate Robust 3D Human Mesh from In-the-Wild Crowded Scenes]

> This work deals with recovering a single person's 3D human mesh from in-the-wild crowded scenes. The method, called 3DCrowNet, is composed of a feature extractor and a joint-based regressor. The feature extractor takes a pose and an image as input. The pose is 2D joint coordinates predicted by bottom-up 2D pose estimators. By making a Gaussian blob on the 2D joint coordinates, the 2D pose is provided as a heatmap representation to the feature extractor. Firstly, the feature extractor obtains an early-stage image feature of ResNet from a cropped image. Then, it concatenates the feature map and the 2D pose heatmap along the channel dimension. The concatenated feature is then processed by a $3\times3$ convolution block to keep the feature's size and change the channel dimension. At last, the feature is fed back to the remaining part of ResNet to produce a crowded scene-robust image feature. The joint-based regressor recovers 3D joint coordinates from the scene-robust image feature. In detail, The crowded scene-robust image feature is processed by a 1-by-1 convolutional layer to predict a 2D feature map and then reshaped to a 3D heatmap. The 3D joint coordinates are then computed from the 3D heatmap by the soft-argmax operation. Then, the joint-based regressor estimates global rotation $\theta^g$ of a person, SMPL body rotation parameters $\theta$, SMPL shape parameters $\beta$, and camera parameters $k$ for projection. For each joint, the scene-robust image feature, 3D coordinates and the prediction confidence corresponds to the coordinates are concatenated to obtain another feature $F^M$. This feature is processed by a GCN, the output features of GCN are used to predict the global rotation, VPoser latent code $\z, \beta$ and $k$. This method is trained on Human3.6M, MuCo-3DHP, MSCOCO, MPII, and CrowdPose datasets. For evaluation, this method also proposes the 3DPW-Crowd dataset to numerically measure a method’s robustness on in-the-wild crowded scenes.

- [x] [**arXiv 2022**] [Confidence Score for Source-Free Unsupervised Domain Adaptation]

> This work deals with the ***source-free unsupervised domain adaptation*** problem. Unlike the unsupervised domain adaptation (UDA) problem, which assume both the sourece data and corresponding labels are available, the pre-trained model using labeled soure domain data is only accessable for the SFUDA task. Existing SFUDA methods are vulnerable to incorrect pseudo-labels and confirmation bias. This paper proposed a Joint Model-Data Structure (JMDS) score for SFUDA, which is the first confidence score that can utilize both source and target domain knowledge. The JDMS score consists of a Log Probability Gap (LPG) score and a Model Probability of Pseudo-Label (MPPL) score, where the Caussian Mixture Modeling (GMM) in the target feature space is used to obtain the log-likelihood and pseudo-label for each sample. To use the JMDS score in the learning process, this work proposed a Confidence Score Weighting Adaptiation using the JMDS framework that uses the JMDS score as a sample-wise weight and pseudo-labels obtained from GMM. The JSMS score and CoWA-JMDS are evaluated on various public UDA benchmarks and achieved the best performance in terms of the measuring confidence.

- [x] [BANMo: Building Animatable 3D Neural Models from Many Casual Videos]

> This work proposed a method called BANMo that combines classic deformable shape models, canonical embeddings and NeRF for articulated 3D shape reconstruction. Specifically, a 3D point in the canonical space is associated with three properties: color, density, and a canonical embedding. The color prediction follows a NeRF-like MLP, the density is converted from a MLP which computes the signed distance function of a point to the surface, the feature embedding is computed by another individual MLP and the embedding can be used to match 3D points across different viewpoints and lighting conditions. One of the key components of BANMo is the space-time forward and backward warping function, which maps 3D points between canonical space and camera space at timestamp t. BANMo represents this 3D warps as compositions of neural-weighted rigid-body transformations, where the rigid transformations J that moves bones from their rest configuration to deformed states are weighted by the pose-dependent skinning weights. During the transformation, the root body pose G and J are regressed from MLP. The skinning weights are conditioned on explicit 3D Gaussian ellipsoids that move along with the bone coordinates. To register pixel observations at different timestamps, BANMo uses canonical feature embeddings that encode semantic information of 3D points in the canonical space and the candidate 3D point is found by matching the feature embedding to the pixel feature embedding. The model is optimized over reconstruction losses and feature registration losses and a 3D cycle consistency loss. The quantitative and qualitative experiments are respectively conducted on the AMA human dataset and the animated objects dataset, and BAnMo outperforms ViSER and Nerfies by a large margin.