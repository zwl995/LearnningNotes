# Weekly Papers
----------------------------------------------------

- [x] [**ICCV 2021** | **[Github](https://github.com/ChrisWu1997/DeepCAD)**] [DeepCAD: A Deep Generative Network for Computer-Aided Design Models](https://arxiv.org/pdf/2105.09492.pdf)


> This paper presents a deep generative method that outputs the sequence of operations used in CAD tools. DeepCAD considers the most commonly used CAD operations, and unifies them into a common structure that encodes the operation types, parameters and sequential orders. It uses a Transformer-based autoencoder to embed the CAD model into a latent space, and decodes the latent vector into a CAD command sequence. This work also generates a large amount of CAD models by Onshape to train their autoencoder. (The CAD model has two levels of representation: one is the user-interaction level, one the the boundary representation(Brep).)
In DeepCAD, a CAD model is described by a sequence of curve commands interleave with extrusion commands. To overcome the issue that CAD commands differ from the natural language, DeepCAD represents each command into a 16-dimensional vectors, and also fixed the total number of commands in every CAD model. It also unified continuous and discrete parameters by quantizing the continuous parameters, which can improve the generation quality. Leveraging the representation of CAD commands, the autoencoder takes as input a CAD command sequence, and projects commands into the embedding space, then feds the embeddings into an encoder to output a latent vector. At last, DeepCAD used the latent-GAN to automatically generate editable CAD models. The proposed method is benchmarked based on the Command Accuracy (ACCcmd) and Parameter Accuracy (ACCparam). 
The experiments also show the effectiveness the data representation. However, this method performs not good on CAD models that require long commands to describe.

Q: Why failed when command sequences becomes quite long?
A: Not enough training data for long sequences.

- [x] [**CVPR 2021** | **[Github]()**] [uncertainty guided collaborative training for weakly supervised temporal action detection](https://openaccess.thecvf.com/content/CVPR2021/papers/Yang_Uncertainty_Guided_Collaborative_Training_for_Weakly_Supervised_Temporal_Action_Detection_CVPR_2021_paper.pdf)

> This paper deals with the temporal action detection problem. The work proposed an ***Uncertainty Guided Collaborative Training(UGCT)*** strategy that enables the online generation of pseudo ground truth labels and an uncertainty aware learning module that can mitigate the noise in the generated pseudo labels. In the pseudo ground truth generation module, the pseudo labels are generated by a teacher model. The RGB stream and FLOW stream work collaboratively that can learn from each other, as the teacher model in the RGB stream provides pseudo labels for the FLOW stream and the teacher model in the FLOW stream provides pseudo labels for the RGB stream. In the uncertainty prediction module, the uncertainty about the pseudo label is predicted. And this module is removed after training.


- [x] [**ICCV 2021 oral** | **[Github](https://github.com/autonomousvision/unisurf)**] [UNISURF: Unifying Neural Implicit Surfaces and Radiance Fields for Multi-View Reconstruction](http://www.cvlibs.net/publications/Oechsle2021ICCV.pdf)

> This work deals with the neural implicit rendering problem.
The rendering technique of ***neural implicit multi-view reconstruction*** has two mainstream catagories: *surface rendering technique* and *volume rendering technique*. Surface rendering technique can estimate surface geometry accurately, while requires per-pixel object masks and appropriate network initialization, which is limited to small scenes. Volume rendering techniques like NeRF require no input masks, while can only extract surface as level sets of the underlying volume density, which are usually non-smooth and contain artifacts.
UNISURF proposed an unified formulation which combines both the advantages of surface rendering and volume rendering, enabling the reconstruction of accurate geometry from multi-view images without masks. And it adopted a $l_1$ reconstruction loss with a $l_2$ surface regularization term. During optimization, the sampling interval for drawing samples during volume rendering is monotonically decreased. UNISURF is evaluated on the DTU, blended MVS datasets and SceneNet dataset, and achieves state-of-the-art. However, this work is limited on solid and non-transparent surfaces. Besides, the overexposed and texture-less areas can also limit UNISURF's performance, and the reconstructions are less accurate at rarely visible regions in the images. (My question: can this method adapts to larger areas, i.e, city-scale aerial images?)

- [x] [**ECCV 2020**] [Unsupervised Shape and Pose Disentanglement
for 3D Meshes](https://arxiv.org/pdf/2007.11341.pdf)

> The paper addresses the problem of unsupervised disentanglement of
pose and shape for 3D meshes. It used an auto-encoder neural network to auto-encode a mesh in *pose* code and *shape* code. Built upon the cross-consistency and self-consistency. To achieve the cross-consistency constraints, this work generates pairs of different shapes with the exact same pose on the fly during training with our disentangling network. The self-consistency is achieved by generating a proxy mesh $\tilde{X}^t$ with the pose of mesh $X^s_1$ and the shape of mesh $X_t$ within the training loop from two meshes with different shapes and poses $X^s_1$ and $X^t$. An effective disentanglement should recover the original pose code from the proxy mesh. Accordingly, the cross-consistency loss and self-consistency loss are proposed to train the network. At last, this work used the *as-rigid-as-possible Deformation* algorithm to deform $X^t$ to match the pose of the network prediction $\tilde{X}^t$ while preserving the original shape as much as possible. Their model is straightforward to train and it generalizes well across various datasets.
The limitation is that they have no control over specific parts of the mesh with their pose code. And also the interpolation of large torso rotations squeezes the meshes.

- [x] [**NeurIPS 2021** | **[Github](https://github.com/TorchSSL/TorchSSL)**] [FlexMatch: Boosting Semi-Supervised Learning with Curriculum Pseudo Labeling]()

> This paper deals with the semi-supervised learning(SSL) problem. The main contribution of this work is to propose the Curriculum Pseudo Labeling(CPL) approach, which takes into account the learning status of each class for semi-supervised learning. As modern SSL algorithms such as Pseudo-Labeling and Unsupervised Data Augmentation(UDA) rely on a fixed threshold to compute the unsupervised loss, and can only handle all classes equally without considering the different learning difficulties, this work used a initial threshold which can be dynamically adjusted during training. when apply the CPL module into FixMatch, the called FlexMatch method converges faster than FixMatch and achieves SOTA performances on most SSL image classification benchmarks. Specifically, FixMatch uses consistency regularization and data augmentation to improve the training performance, where the unsupervised loss needs a pre-defined threshold. The intuition of Curriculum Pseudo Labeling is that, when the threshold is high, the learning effect of a class can be reflected by the number of samples whose predictions fall into this class and above the threshold. Based on the intuition, CPL proposed a new loss, where the threshold of a well-learned class is raised higher to selectively pick up higher-quality samples as learning proceeds. Besides, more techniques such as threshold warm-up and nonlinear mapping are proposed to improve the performance.


- [x] [**NeurIPS 2021**] [Adversarial Teacher-Student Representation Learning for Domain Generalization](https://papers.nips.cc/paper/2021/file/a2137a2ae8e39b5002a3f8909ecb88fe-Paper.pdf)

> This work aims at solving the **domain generalization** problem. Different from domain adaptation (DA), which observes both source
and target-domain training data for performing learning tasks across domains, domain generalization trains models using data observed from single or multiple source domains to expect generalizing the model to unseen target domains. Unlike most existing domain generalization approaches, which focus on deriving **domain-invariant features** among multiple source domains or adopting meta-learning techniques, this work proposed a unique Adversarial Teacher-Student Representation Learning framework. The core parts of the method are the **Domain Generalized Representation Learning** module and the **Novel Domain Augmentation** module. In the Domain Generalized Representation Learning module, a teacher-Student learning scheme is proposed to extract the domain generalizable feature representations, where the Student is used for exploring novel-domain augmentation synthesized from the novel-domain
augmenter G, while distilling the associated knowledge to update Teacher, and the teacher can gain more generalizability.  The Novel Domain Augmentation is used to progressively perform novel-domain data augmentation. The aim of this module is to train the novel-domain augmenter G while freezing both teacher and student, so as to maximize the discrepancy between z and $\tilde{z}$. The experiment is conducted on the PACS, Office-Home, Domain-Net, VLCS and Digit-DG datasets, and retrieve remarkably improvement compared to existing methods. The ablation study also shows the effectiveness of the Adversarial Augmenter and the Domain Generalized Teacher.


- [x] [**CVPR 2021**] [Neural Scene Graphs for Dynamic Scenes](http://light.princeton.edu/neural-scene-graphs)

> This work deals with the neural rendering problem under dynamic scenes. Existing representative works like NeRF can render static scenes, however, are not able to render dynamic scenes and decompose scenes into individual objects. To solve the problem, this work represents scenes by a learned scene graph, where the static objects and dynamics objects are nodes, and transformations and scaling factors are edges. Based on NeRF, this work proposes to augment the implicit neural radiance fields when renders background node and dynamic nodes, where the original positional encoding of inputs is replaced by a Fourier encoding. For background node, the static radiance is represented on sparse planes instead of a volume. For dynamic nodes, objects belong to the same class are represented by a function with same weights to reduce the number of models and also the training time.. Meanwhile, the neural radiance fields is augmented with a ***latent class encoding*** to distinguish individual objects. Besides, the pose is also served as input to the neural radiance fields to ensure volumetric consistency.
At last, the learned scene graph is rendered using a ray casting approach.
To increase efficiency, the sampling at the static node is limited to $N_s$ planes that are parallel to the image plane. For each dynamic node, each ray from the camera for intersections with all dynamic nodes is checked by translating the ray to an object local frame and applying an efficient AABB-ray intersection test. Besides, only a small number of equidistant points 
are enough to accurately represent dynamic objects and maintaining rendering efficiency.
The learned scene graph is optimized based on the predicted color with the latent codes served as regularizers. The experiments are conducted on the KITTI dataset and the virtual KITTI 2 dataset. When compared against the SRN method, NeRF and NeRF with time parameter, this work shows superiority over these methods under dynamic scenes.
